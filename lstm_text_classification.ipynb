{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying News Articles Using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using TorchText, which is PyTorch text library providing advanced functionalities on language processing.\n",
    "\n",
    "The dataset we'll be using is AG News. AG News (AG’s News Corpus) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.\n",
    "\n",
    "**Note:** AG_NEWS is not a PyTorch dataset class; it is a function that returns a PyTorch DataPipe. PyTorch DataPipes are used for data loading and processing in PyTorch.\n",
    "\n",
    "More information on PyTorch DataPipes: https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "\n",
    "Difference between DataSet and DataPipe: https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the AG News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_root = \"../../datasets\"  # Root directory of the dataset\n",
    "dataset_train = AG_NEWS(root=data_set_root, split=\"train\")  # Training dataset\n",
    "dataset_test = AG_NEWS(root=data_set_root, split=\"test\")  # Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4  # Learning rate for the optimizer\n",
    "nepochs = 20  # Number of training epochs\n",
    "batch_size = 32  # Batch size for training\n",
    "max_len = 128  # Maximum length of input sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the process of splitting a text into individual words or tokens.\n",
    "\n",
    "Here, we use the `get_tokenizer` function from the `torchtext.data.utils` module to create a tokenizer based on the \"basic_english\" tokenization method.\n",
    "\n",
    "Then we define a generator function `yield_tokens` to yield tokens from the data iterator.The data iterator provides pairs of (label, text) where text is the input sentence. From these tokens we can build the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "# We set min_freq=2 to include a token only if it appears more than 2 times in the dataset.\n",
    "\n",
    "# We will also add \"special\" tokens that we'll use to signal something to our model\n",
    "# <pad> is a padding token that is added to the end of a sentence to ensure \n",
    "# the length of all sequences in a batch is the same\n",
    "# <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "# <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\n",
    "# <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(dataset_train),  # Tokenized data iterator\n",
    "    min_freq=2,  # Minimum frequency threshold for token inclusion\n",
    "    specials=['<pad>', '<sos>', '<eos>', '<unk>'],  # Special case tokens\n",
    "    special_first=True  # Place special tokens first in the vocabulary\n",
    ")\n",
    "\n",
    "# Set the default index of the vocabulary to the index of the <unk> token.\n",
    "# If a token is not found in the vocabulary, it will be replaced with the <unk> token.\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
