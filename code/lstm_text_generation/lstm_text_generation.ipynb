{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using TorchText, which is PyTorch text library providing advanced functionalities on language processing.\n",
    "\n",
    "The dataset we'll be using is AG News. AG News (AG’s News Corpus) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from torchtext.data.functional import generate_sp_model\n",
    "from torchtext.datasets import WikiText2, EnWik9, AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchtext.data.functional import sentencepiece_tokenizer, load_sp_model\n",
    "from AGNewsDataset import AGNews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4  # Learning rate for the optimizer\n",
    "nepochs = 20  # Number of training epochs\n",
    "batch_size = 32  # Batch size for training\n",
    "max_len = 64  # Maximum length of input sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the AGNews Dataset class, that performs some filtering and error correction on the AGNews dataset CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AGNews dataset instances for training and testing\n",
    "dataset_train = AGNews(\"../../data/train.csv\")\n",
    "dataset_test = AGNews(\"../../data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train a TorchText sentence-piece tokenizer model. The model is trained to segment sentences with the least number of tokens possible, in order to build a vocabulary of arbitrary size.\n",
    "This is a more advanced technique than segmenting words using the space character as separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/train.csv\") as f:\n",
    "    with open((\"./vocab/data.txt\"), \"w\") as f2:\n",
    "        for i, line in enumerate(f):\n",
    "            text_only = \"\".join(line.split(\",\")[1:])\n",
    "            filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # remove newline characters\n",
    "            f2.write(filtered.lower() + \"\\n\")\n",
    "\n",
    "generate_sp_model((\"./vocab/data.txt\"), \n",
    "                  vocab_size=20000, model_prefix='./vocab/spm_ag_news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a generator function `yield_tokens` to yield tokens from the data iterator, and then build the vocabulary out of these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to yield tokens from a file\n",
    "def yield_tokens(file_path):\n",
    "    # Open the file in UTF-8 encoding\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        # Iterate over each line in the file\n",
    "        for line in f:\n",
    "            # Yield the token split by tab character\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "# Build vocabulary from the iterator of tokens\n",
    "# We will also add \"special\" tokens that we'll use to signal something to our model\n",
    "# <pad> is a padding token that is added to the end of a sentence to ensure \n",
    "# the length of all sequences in a batch is the same\n",
    "# <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "# <eos> signals the \"End-Of-Sentence\" aka the end of the sequence\n",
    "# <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(\"./vocab/spm_ag_news.vocab\"),\n",
    "    # Define special tokens with special_first=True to place them at the beginning of the vocabulary\n",
    "    specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "# Set default index for out-of-vocabulary tokens\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a text transformation pipeline using TorchText Sequential Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformation pipeline for training data\n",
    "train_transform = T.Sequential(\n",
    "    # Tokenize sentences using pre-existing SentencePiece tokenizer model\n",
    "    T.SentencePieceTokenizer(\"./vocab/spm_ag_news.model\"),\n",
    "    # Convert tokens to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    # Add <sos> token at the beginning of each sentence (index 1 in vocabulary)\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentence if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len),\n",
    "    # Add <eos> token at the end of each sentence (index 2 in vocabulary)\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor and pad sentences with the <pad> token if shorter than max length\n",
    "    T.ToTensor(padding_value=0)\n",
    ")\n",
    "\n",
    "# Define a transformation pipeline for generation (without truncation)\n",
    "gen_transform = T.Sequential(\n",
    "    # Tokenize sentences using pre-existing SentencePiece tokenizer model\n",
    "    T.SentencePieceTokenizer(\"./vocab/spm_ag_news.model\"),\n",
    "    # Convert tokens to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    # Add <sos> token at the beginning of each sentence (index 1 in vocabulary)\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Convert the list of lists to a tensor and pad sentences with the <pad> token if shorter than max length\n",
    "    T.ToTensor(padding_value=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders for training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for training dataset\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "# DataLoader for testing dataset\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_emb, num_layers=1, emb_size=128, hidden_size=128):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_emb, emb_size)\n",
    "\n",
    "        self.mlp_emb = nn.Sequential(nn.Linear(emb_size, emb_size),\n",
    "                                     nn.LayerNorm(emb_size),\n",
    "                                     nn.ELU(),\n",
    "                                     nn.Linear(emb_size, emb_size))\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.25)\n",
    "\n",
    "        self.mlp_out = nn.Sequential(nn.Linear(hidden_size, hidden_size//2),\n",
    "                                     nn.LayerNorm(hidden_size//2),\n",
    "                                     nn.ELU(),\n",
    "                                     nn.Dropout(0.5),\n",
    "                                     nn.Linear(hidden_size//2, num_emb))\n",
    "        \n",
    "    def forward(self, input_seq, hidden_in, mem_in):\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        input_embs = self.mlp_emb(input_embs)\n",
    "                \n",
    "        output, (hidden_out, mem_out) = self.lstm(input_embs, (hidden_in, mem_in))\n",
    "                \n",
    "        return self.mlp_out(output), hidden_out, mem_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Optimizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(1 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define embedding size and hidden size for the LSTM model\n",
    "emb_size = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "# Define number of layers for the LSTM model\n",
    "num_layers = 2\n",
    "\n",
    "# Create LSTM model instance\n",
    "lstm_generator = LSTM(num_emb=len(vocab), num_layers=num_layers, \n",
    "                      emb_size=emb_size, hidden_size=hidden_size).to(device)\n",
    "\n",
    "# Initialize optimizer with Adam optimizer\n",
    "optimizer = optim.Adam(lstm_generator.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Define the loss function (Cross Entropy Loss)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# List to store training loss during each epoch\n",
    "training_loss_logger = []\n",
    "\n",
    "# List to store entropy during training (for monitoring)\n",
    "entropy_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in lstm_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):\n",
    "    # Set LSTM generator model to training mode\n",
    "    lstm_generator.train()\n",
    "    steps = 0\n",
    "    # Iterate over batches in training data loader\n",
    "    for text in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        # Transform text tokens using training transform and move to device\n",
    "        text_tokens = train_transform(list(text)).to(device)\n",
    "        bs = text_tokens.shape[0]\n",
    "        \n",
    "        input_text = text_tokens[:, 0:-1]\n",
    "        output_text = text_tokens[:, 1:]\n",
    "        \n",
    "        # Initialize the memory buffers\n",
    "        hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        \n",
    "        # Forward pass through the LSTM generator\n",
    "        pred, hidden, memory = lstm_generator(input_text, hidden, memory)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(pred.transpose(1, 2), output_text)\n",
    "        \n",
    "        # Zero gradients, perform backward pass, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log training loss\n",
    "        training_loss_logger.append(loss.item())\n",
    "        \n",
    "        # Log entropy during training (for monitoring)\n",
    "        with torch.no_grad():\n",
    "            dist = Categorical(logits=pred)\n",
    "            entropy_logger.append(dist.entropy().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can exploit the fact that the articles have the title and the content separated by a column. So we can provide a title as prompt and generate some content based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some test examples\n",
    "text = next(iter(data_loader_test))\n",
    "\n",
    "# Choose an index from the test data loader\n",
    "index = 0\n",
    "\n",
    "# Temperature parameter for sampling: by changing this value we can adjust the randomness of sampling\n",
    "temp = 0.9\n",
    "\n",
    "# We use an example from the test set\n",
    "init_prompt = [text[index].split(\":\")[0] + \":\"]\n",
    "\n",
    "# Transform the initial prompt into tokens and move to device\n",
    "input_tokens = gen_transform(init_prompt).to(device)\n",
    "\n",
    "print(\"INITIAL PROMPT:\")\n",
    "print(init_prompt[0])\n",
    "\n",
    "print(\"\\nPROMPT TOKENS:\")\n",
    "print(input_tokens)\n",
    "print(vocab.lookup_tokens(input_tokens[0].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_tokens = []\n",
    "\n",
    "# Set LSTM generator model to evaluation mode\n",
    "lstm_generator.eval()\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():    \n",
    "    # Initialize hidden and memory states\n",
    "    hidden = torch.zeros(num_layers, 1, hidden_size, device=device)\n",
    "    memory = torch.zeros(num_layers, 1, hidden_size, device=device)\n",
    "    \n",
    "    # Generate text\n",
    "    for i in range(100):\n",
    "        # Forward pass through LSTM generator\n",
    "        data_pred, hidden, memory = lstm_generator(input_tokens, hidden, memory)\n",
    "        \n",
    "        # Sample from the distribution of probabilities (with temperature)\n",
    "        dist = Categorical(logits=data_pred[:, -1] / temp)\n",
    "        input_tokens = dist.sample().reshape(1, 1)\n",
    "        \n",
    "        # Append generated token to log_tokens\n",
    "        log_tokens.append(input_tokens.cpu())\n",
    "        \n",
    "        # Check for end-of-sentence token\n",
    "        if input_tokens.item() == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, combine the model's output with the initial title to get our article!\n",
    "init_prompt[0] + pred_text.replace(\"▁\", \" \").replace(\"<unk>\", \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
