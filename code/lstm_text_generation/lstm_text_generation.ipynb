{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from torchtext.data.functional import generate_sp_model\n",
    "from torchtext.datasets import WikiText2, EnWik9, AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchtext.data.functional import sentencepiece_tokenizer, load_sp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4  # Learning rate for the optimizer\n",
    "nepochs = 20  # Number of training epochs\n",
    "batch_size = 32  # Batch size for training\n",
    "max_len = 64  # Maximum length of input sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the AGNews Dataset class, that performs some filtering and error correction on the AGNews dataset CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGNews dataset class definition\n",
    "class AGNews(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        # Read the AG News dataset CSV file\n",
    "        self.df = pd.read_csv(filename, names=[\"Class\", \"Title\", \"Content\"])\n",
    "        \n",
    "        # Fill missing values with empty string\n",
    "        self.df.fillna('', inplace=True)\n",
    "        \n",
    "        # Combine Title and Content columns into a single Article column\n",
    "        self.df['Article'] = self.df['Title'] + \" : \" + self.df['Content']\n",
    "        \n",
    "        # Drop Title and Content columns as they are no longer needed\n",
    "        self.df.drop(['Title', 'Content'], axis=1, inplace=True)\n",
    "        \n",
    "        # Replace special characters with whitespace in the Article column\n",
    "        self.df['Article'] = self.df['Article'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "\n",
    "    # Method to get a single item from the dataset\n",
    "    def __getitem__(self, index):\n",
    "        # Get the text of the article at the given index, converted to lowercase\n",
    "        text = self.df.loc[index][\"Article\"].lower()\n",
    "\n",
    "        return text\n",
    "\n",
    "    # Method to get the length of the dataset\n",
    "    def __len__(self):\n",
    "        # Return the total number of articles in the dataset\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AGNews dataset instances for training and testing\n",
    "dataset_train = AGNews(\"../../data/train.csv\")\n",
    "dataset_test = AGNews(\"../../data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train a sentence-piece tokenizer model. The model is trained to segment sentences with the least number of tokens possible, in order to build a vocabulary of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=./vocab/data.txt --model_prefix=./vocab/spm_ag_news --vocab_size=20000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./vocab/data.txt\n",
      "  input_format: \n",
      "  model_prefix: ./vocab/spm_ag_news\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: ./vocab/data.txt\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 120001 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=28190724\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9755% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=48\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999755\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 120001 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 188413 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 120001\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 141913\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 141913 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=65207 obj=11.3426 num_tokens=323341 num_tokens/piece=4.95869\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=52633 obj=9.14953 num_tokens=324123 num_tokens/piece=6.15817\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=39471 obj=9.10745 num_tokens=335985 num_tokens/piece=8.5122\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=39457 obj=9.09438 num_tokens=336160 num_tokens/piece=8.51965\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=29592 obj=9.13367 num_tokens=358263 num_tokens/piece=12.1068\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=29592 obj=9.12222 num_tokens=358227 num_tokens/piece=12.1055\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=22194 obj=9.19719 num_tokens=384692 num_tokens/piece=17.3332\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=22193 obj=9.17844 num_tokens=384645 num_tokens/piece=17.3318\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=22000 obj=9.17981 num_tokens=385279 num_tokens/piece=17.5127\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=22000 obj=9.17914 num_tokens=385284 num_tokens/piece=17.5129\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: ./vocab/spm_ag_news.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: ./vocab/spm_ag_news.vocab\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/train.csv\") as f:\n",
    "    with open((\"./vocab/data.txt\"), \"w\") as f2:\n",
    "        for i, line in enumerate(f):\n",
    "            text_only = \"\".join(line.split(\",\")[1:])\n",
    "            filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # remove newline characters\n",
    "            f2.write(filtered.lower() + \"\\n\")\n",
    "\n",
    "generate_sp_model((\"./vocab/data.txt\"), \n",
    "                  vocab_size=20000, model_prefix='./vocab/spm_ag_news')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
